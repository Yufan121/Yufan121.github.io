<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="Yufan Xia">







<title>Lab5 | This is Yufan Xia</title>



    <link rel="icon" href="/favicon.ico">



<style>
    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=Noto+Sans+SC:wght@300;400;500;700&family=Roboto+Mono&display=swap');
</style>



    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    




    <!-- scripts list from _config.yml -->
    
    <script src="/js/frame.js"></script>
    




    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>







  <!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head>
  <body>
    <div class="mask-border">
    </div>

    <div class="wrapper">

      <div class="header">
  <div class="flex-container">
    <div class="header-inner">
      <div class="site-brand-container">
        <a href="/">
          
            Yufan Xia.
          
        </a>
      </div>
      <div id="menu-btn" class="menu-btn" onclick="toggleMenu()">
        Menu
      </div>
      <nav class="site-nav">
        <ul class="menu-list">
          
            
              <li class="menu-item">
                <a href="/">Home</a>
              </li> 
                   
          
            
              <li class="menu-item">
                <a href="/Publications/">Publications</a>
              </li> 
                   
          
            
              <li class="menu-item">
                <a href="/archives/">Blogs</a>
              </li> 
                   
          
        </ul>
      </nav>
    </div>
  </div>
</div>


      <div class="main">
        <div class="flex-container">
          <article id="post">

  
    <div class="post-head">
    <div class="post-info">
        <div class="tag-list">
            
        </div>
        <div class="post-title">
            
            
                Lab5
            
            
        </div>
        <span class="post-date">
            May 4, 2022
        </span>
    </div>
    <div class="post-img">
        
            <div class="h-line-primary"></div>
              
    </div>
</div>
    <div class="post-content">
    <p>Lab 5 for COMP4300 - CUDA and GPU Programming</p>
<h1 id="Lab-Tasks"><a href="#Lab-Tasks" class="headerlink" title="Lab Tasks"></a>Lab Tasks</h1><p>Welcome to the fifth lab for COMP4300! Today, we will take a brief look at programming using the CUDA API discussed in lectures, enabling us to harness the considerable power of GPU accelerators.</p>
<p>Firstly, a quick overview of the systems available to you with GPUs installed.</p>
<ul>
<li>The easiest system to access (and what we recommend you complete this lab on) is the <code>stugpu2.anu.edu.au</code> server administered by cecs. You can access this server via ssh in a similar fashion to <code>partch</code> (<a target="_blank" rel="noopener" href="https://cs.anu.edu.au/docs/student-computing-environment/linuxlabs/remoteaccess/">instructions here</a>), using your ANU id as your username. <em>NB:</em> We have seen students struggle with firewall issues connecting to this server - if this happens to you, you should try and log in to <code>partch.anu.edu.au</code> first, and then log in to <code>stugpu2.anu.edu.au</code> once logged into <code>partch</code>.</li>
<li>Gadi has a special queue available for GPU nodes, called <code>gpuvolta</code> (see <a target="_blank" rel="noopener" href="https://opus.nci.org.au/display/Help/Queue+Structure">the queue structure page</a>). All the normal caveats about using Gadi apply - in particular, make sure to limit the walltime for any jobs you submit. You will also need to enter a number of GPUs for the job to utilise. One additional restriction is that you have to request 12 times as many CPUs as GPUs; so if you request 2 GPUs you need 24 CPUs.</li>
<li>If your own computer has a GPU, you can run all of the CUDA code provided on that card! NVIDIA provides relatively good instructions on how to install the CUDA compilers <a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-quick-start-guide/index.html">here</a> - if you have conda installed you can do it in a single command!</li>
</ul>
<p>As mentioned in the list, we recommend that you use <code>stugpu2</code> for this lab - it allows you to focus on GPU programming without having to worry about Gadi queues, which are known to be quite long for the GPUs. </p>
<h2 id="Hello-GPU"><a href="#Hello-GPU" class="headerlink" title="Hello GPU!"></a>Hello GPU!</h2><p>To start with, let’s take a simple Hello, World program to introduce ourselves to the compilation tools, and look at how to make a kernel call. The following code may be found in <code>hello_cuda.cu</code> (the .cu extension is used for files containing CUDA code).</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">Hello</span><span class="params">(<span class="type">void</span>)</span> </span>&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Hello from thread %d in block %d\n&quot;</span>, threadIdx.x, blockIdx.x);</span><br><span class="line">&#125; <span class="comment">/* Hello */</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span>* argv[])</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> blk_ct;       <span class="comment">// Number of thread blocks</span></span><br><span class="line">    <span class="type">int</span> th_per_block; <span class="comment">// Number of threads per block</span></span><br><span class="line"></span><br><span class="line">    blk_ct = <span class="built_in">strtol</span>(argv[<span class="number">1</span>], <span class="literal">NULL</span>, <span class="number">10</span>);</span><br><span class="line">    th_per_block = <span class="built_in">strtol</span>(argv[<span class="number">2</span>], <span class="literal">NULL</span>, <span class="number">10</span>);</span><br><span class="line"></span><br><span class="line">    Hello &lt;&lt;&lt;blk_ct, th_per_block&gt;&gt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;  <span class="comment">/* main */</span></span><br></pre></td></tr></table></figure>
<p>We utilise the <strong>NV</strong>idia <strong>C</strong>uda <strong>C</strong>ompiler (<em>nvcc</em>) to compile this code in a way that makes sense to the GPU and the host machine. Specifically, you can write<br><code>nvcc -o hello_cuda hello_cuda.cu</code>  </p>
<p>You might notice this program has additional runtime arguments provided to main - these can be provided in the standard way, e.g <code>./hello_cuda 4 5</code> (for a program execution with 4 blocks of threads with 5 threads per block).  </p>
<p>Of interest in this program as well is the <code>__global__</code> specifier in front of the <code>Hello</code> function. This indicates that the code in this file can be called from either the host (the CPU) or the device (the GPU), and is executed on the device. Such functions (known as a kernel) also require an execution context, which is provided on this line <code>Hello &lt;&lt;&lt;blk_ct, th_per_block&gt;&gt;&gt;();</code> The other specification options available are <code>__host__</code> and <code>__device__</code>. </p>
<p>You should compile and run this program with a couple of different arguments to check that your chosen method of accessing a GPU is working correctly.  </p>
<p>A couple of questions follow to check your understanding of basic CUDA concepts from the lectures; try answering them first, and then check that your answers are correct by experimenting with the program. As always, ask your tutor lots of questions if you’re not sure of an answer!  </p>
<ul>
<li>If you were to comment out the call to <code>cudaDeviceSynchronize();</code>, what would happen?</li>
<li>What is the largest number of threads you can start? What happens if you exceed that number?</li>
<li>For small values of <code>blk_ct</code> and <code>th_per_block</code> (try 4,4), you will notice that the thread outputs within a block are always ordered; thread 0 says hello, then thread 1, etc. Is this always the case? If not, what is the smallest value of <code>th_per_block</code> that we can provide to get outputs that are not ordered within a block?</li>
</ul>
<h2 id="Vector-Addition-and-Scheduling"><a href="#Vector-Addition-and-Scheduling" class="headerlink" title="Vector Addition and Scheduling"></a>Vector Addition and Scheduling</h2><p>Hello world is great, but having covered off how to compile and run a CUDA program it’s time to move on to more complex applications. Specifically, we’re going to start off with a program to implement vector addition. In programming terms, we’re going to take two arrays <code>x</code> and <code>y</code> of size <code>n</code>, and return another array <code>z</code>, also of size <code>n</code> where for all indices <code>i</code>, <code>z[i] = x[i] + y[i]</code>.<br>You will find in the file <code>vector_add.cu</code> a number of functions to assist in the serial parts of this task: initialising the arrays with random values, a serial function to perform the vector addition on the CPU, and a <code>Two_norm_diff</code> function, which computes the average difference between the GPU output and the CPU output. The mathematical details of this function are not important, other than that if your code is correct, the two-norm difference should be 0.<br>What is missing is the body of three functions:</p>
<ul>
<li>Allocate_vectors</li>
<li>Free_vectors</li>
<li>Vec_add</li>
</ul>
<p>The <code>Allocate_vectors</code> and <code>Free_vectors</code> functions are self-explanatory; there are four vectors in this program of length <code>n</code>. <code>x</code>, <code>y</code>, and <code>z</code> need to be exposed to the GPU and the CPU, while <code>cz</code> only needs to exist in CPU memory, as <code>cz</code> will contain the result of the CPU computing <code>z</code>. You can do this by utilising the <code>cudaMallocManaged</code>, and <code>cudaFree</code>, as well as standard <code>malloc</code> and <code>free</code> calls. Further details on how to use these methods can be found in lectures, and in the man pages accessible from your terminal using (eg) <code>man cudaMallocManaged</code>.  </p>
<p>The <code>Vec_add</code> method is the main kernel of this code, and where you will put the  work of computing <code>z</code> on the GPU. The idea here is to give each thread a single index to operate on (for now you can assume that the number of threads is equal to the number of elements in the arrays). So, you will need to decide which index the current thread should work on, calculate the correct value at that index, and write it to <code>z</code>. Recall that to help you with this task, you have the <code>threadIdx</code>, <code>blockIdx</code>, <code>gridDim</code>, and <code>blockDim</code> of the thread at hand.  </p>
<p>Your first task is to fill in these functions such that the GPU returns the correct sum into <code>z[]</code>, which will be evidenced by compiling and running the program, and it telling you the two-norm of difference is 0.  </p>
<p>Having got the code correctly computing the sum of two vectors, the next thing to do is to relax the assumption we made earlier that the number of threads is equal to the number of elements in each array. There are many different ways of doing this, but the basic idea is to somehow choose multiple indices that each thread should operate on, and then loop through those indices.<br>Two common ways of choosing this are to decide how many elements each thread should operate on, and then giving each thread a block of that size. For instance, if each thread gets 3 elements, thread (0,0) should operate on indices 0,1, and 2. This is an example of a block distribution<br>Another way of doing it would be to compute the total number of threads, and then use a cyclic distribution, so if there are 4 threads in total, then thread (0,0) operates on elements 0,4,8,…<br>Decide on one of these methods (or try implementing both), and implement it into your <code>Vec_add</code> kernel, so that the program is capable of handling cases where <code>n</code> is larger or smaller than the total number of threads available.</p>
<h2 id="Trapezoids-Trees-and-Warp-Shuffles"><a href="#Trapezoids-Trees-and-Warp-Shuffles" class="headerlink" title="Trapezoids, Trees, and Warp Shuffles"></a>Trapezoids, Trees, and Warp Shuffles</h2><p>For the final part of this lab, we will move away from looking at getting correct parallel algorithms, and instead measure the walltime taken to compute the integral of some function <code>f</code> over an interval <code>a</code> to <code>b</code> using the trapezoidal rule.<br>For those not familiar, the trapezoidal rule &lt;insert description of the trapezoidal rule’s mathematics&gt;.<br>For those of us who prefer to think in code rather than mathematics, here’s a serial implementation of the trapezoidal rule, which can be found alongside some other serial functions in the file <code>trap.cu</code>:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">float</span> <span class="title">Serial_trap</span><span class="params">(<span class="type">const</span> <span class="type">float</span> a, <span class="type">const</span> <span class="type">float</span> b, <span class="type">const</span> <span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="type">float</span> x,h = (b-a)/n;</span><br><span class="line">    <span class="type">float</span> trap = <span class="number">0.5</span>*(<span class="built_in">f</span>(a) + <span class="built_in">f</span>(b));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;=n<span class="number">-1</span>; i++) &#123;</span><br><span class="line">        x = a + i*h;</span><br><span class="line">        trap += <span class="built_in">f</span>(x);</span><br><span class="line">    &#125;</span><br><span class="line">    trap = trap*h;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> trap;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Note that for our program, the number of trapezoids used in the approximation is parameterised as <code>n</code>.   </p>
<p>As is hopefully becoming more familiar, the first part of improving the performance of a program like this with CUDA is to write a device kernel. Somewhat unhelpfully, the code for the kernel has not been provided to you, so your first task is to write the kernel function <code>Dev_trap</code>. Two gotchas to be aware of here:</p>
<ul>
<li>You can’t just add to the float pointed to with <code>trap_p</code> directly, as that’ll create a race condition. Helpfully, CUDA provides us with a function <code>atomicAdd()</code> that you should use for this task. It takes a pointer to a shared variable as the first argument, and a numeric value to add to it as the second.</li>
<li>Reading the <code>Trap_wrapper</code> code carefully, you’ll notice that <code>trap_p</code> is already filled in for the first value; make sure not to compute the 0th trapezoid again inside your kernel!<br>You may assume that the number of threads will always equal <code>n</code> - no need to create a scheduling system this time!</li>
</ul>
<p>Now that you’ve got a nice shiny kernel, you should try running it for a number of values for <code>n</code>, and with a couple of different block and thread counts. <code>a</code> and <code>b</code> aren’t super important, because we’re not checking for correctness, but -3 and 3 respectively are good values.<br>Your values may vary, but one of your tutors got these average times for a couple of runs of the program:</p>
<table>
<thead>
<tr>
<th>n</th>
<th>blk_count</th>
<th>th_per_blk</th>
<th>Serial Time (s)</th>
<th>Parallel Time (s)</th>
</tr>
</thead>
<tbody><tr>
<td>256</td>
<td>16</td>
<td>16</td>
<td>0.0000114</td>
<td>0.000218</td>
</tr>
<tr>
<td>1024</td>
<td>32</td>
<td>32</td>
<td>0.0000515</td>
<td>0.000246</td>
</tr>
<tr>
<td>4096</td>
<td>64</td>
<td>64</td>
<td>0.000182</td>
<td>0.000243</td>
</tr>
<tr>
<td>16384</td>
<td>128</td>
<td>128</td>
<td>0.000704</td>
<td>0.000261</td>
</tr>
<tr>
<td>65536</td>
<td>256</td>
<td>256</td>
<td>0.00173</td>
<td>0.000375</td>
</tr>
<tr>
<td>262144</td>
<td>512</td>
<td>512</td>
<td>0.00330</td>
<td>0.000737</td>
</tr>
<tr>
<td>1048576</td>
<td>1024</td>
<td>1024</td>
<td>0.00879</td>
<td>0.00227</td>
</tr>
</tbody></table>
<p>These results show a speed-up, especially for larger <code>n</code>, but getting that speedup requires using 1,048,576 cores over the 1 core the CPU gets! Clearly we can do better.</p>
<h3 id="Warp-Shuffles"><a href="#Warp-Shuffles" class="headerlink" title="Warp Shuffles"></a>Warp Shuffles</h3><p>As you’ve probably guessed by now, the problem with this kernel is that every single thread has to synchronise with every other thread to write their contribution to the overall sum into the shared variable <code>trap_p</code>. So, if we want to improve our function, we need to reduce the number of atomic operations involved. As you’ve seen in lectures, the core unit of execution on an NVIDIA GPU is a warp, and helpfully for us, there are some built-in functions that allow threads inside a warp to talk to each other.<br>Specifically, we’re interested in the warp shuffle - a tree based communication method. In this method, threads within a warp pair off with each other, with one thread sending data, and the other thread receiving it. This is implemented in CUDA through the builtin function <code>__shfl_down_sync</code>, and the below diagram (taken from the course textbook) demonstrates how this can work in a loop to make a global sum when you sum the values read from the other threads.<br>&lt;<Insert Figure>&gt;<br>This figure is for a hypothetical system with a warp size of 8 (our GPUs have a warp size of 32) for readability. To understand how this allows for a global summing, we can trace the values that are recived by thread 0.<br>On the first iteration, it receives a value from thread 4.<br>On the second iteration, it receives a value from combining threads 6 and 2.<br>On the third iteration, it receives a value from combining threads 1 and 3. Thread 1 has a value from combining threads 1 and 5, and thread 3 has a value from combining threads 3 and 7, so essentially we have the information from threads 1,3,5, and 7 on the third iteration.<br>At this point, thread 0 has information from every other thread in the warp, so it has the global sum.  </p>
<p>To get a little more technical, the signature of <code>__shfl_down_sync</code> is as follows:  </p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">__device__ <span class="type">float</span> __shfl_down_sync(</span><br><span class="line">  <span class="type">unsigned</span> mask,</span><br><span class="line">  <span class="type">float</span>    var,</span><br><span class="line">  <span class="type">unsigned</span> diff,</span><br><span class="line">  <span class="type">int</span>      width = warpsize</span><br><span class="line">);</span><br></pre></td></tr></table></figure>
<p>The <code>mask</code> variable specifies which of the 32 threads in the warp are participating in this communication; it is a 32-bit value read in binary. So, if you want all threads to participate, thats <code>0xFFFFFFFF</code>, if you want only the first thread to participate, that’s <code>0x00000001</code>, etc.<br>The <code>var</code> variable is simply the variable that is being communicated to the other threads.<br>The <code>diff</code> variable is the distance in the thread block that we’re communicating by. So if thread 0 should talk to thread 4, you should set <code>diff=4</code>.<br>Finally, the <code>width</code> variable (which is optional) specifies the width of the communication. You will rarely want to change this from just being the width of a warp.<br>The value that is returned from this call is the corresponding value from the thread that we are paired with. So if we’re thread 0 paired with thread 4, the float returned will be the <code>var</code> variable held by thread 4.</p>
<p>Your final task for this lab is to modify your <code>trap.cu</code> file to utilise the warp shuffle method outlined above to obtain an improvement in the computation time taken by the CUDA trap function. Some pointers are below:</p>
<ul>
<li>It is highly recommended that you define another function to do the summation <code>Warp_sum</code> with a signature like <code>__device__ float Warp_sum(float var)</code>. Get this working to sum up variables in a warp, and then think about how to use it to speed up your <code>Dev_trap</code> kernel after.</li>
<li>Remember that only one thread will have the correct result from the <code>Warp_sum</code> call - don’t add the result from every thread to <code>trap_p</code>!</li>
<li>You may assume that thread blocks will contain only one warp. As such, you should only test this code with <code>th_per_blk=32</code>.</li>
</ul>
<h1 id="Take-home-Tasks"><a href="#Take-home-Tasks" class="headerlink" title="Take-home Tasks"></a>Take-home Tasks</h1><p>For this week’s take-home tasks, we will be exploring the performance of a program to compute the dot product of two vectors. This is a reduction operation on two vectors <code>x</code> and <code>y</code>, where <code>dot=x[0]*y[0] + x[1]*y[1] + x[2]*y[2]...</code> for all indices of the two vectors. As usual, serial code to implement this is provided, and it looks like this:  </p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">float</span> <span class="title">Serial_dot_prod</span><span class="params">(<span class="type">const</span> <span class="type">float</span> x[], <span class="type">const</span> <span class="type">float</span> y[], <span class="type">const</span> <span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="type">float</span> cdot = <span class="number">0.0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">        cdot += x[i] * y[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> cdot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="Exercise-1-Basic-CUDA-Implementation-1-mark"><a href="#Exercise-1-Basic-CUDA-Implementation-1-mark" class="headerlink" title="Exercise 1 - Basic CUDA Implementation (1 mark)"></a>Exercise 1 - Basic CUDA Implementation (1 mark)</h2><p>As in the lab exercises, you’ve been provided in <code>dot_prod.cu</code> a file with serial functions and some helper functions to manage inputs and outputs. Fill in this file with three things:</p>
<ul>
<li>A kernel that will compute the dot product in parallel. You should use atomicAdd, and may assume that the number of threads always equals <code>n</code>.</li>
<li>Functions to allocate and de-allocate the memory used in your kernel. Note carefully that one of the values you’ll need to pass into your kernel is not an array, and is just a single float.</li>
<li>Timing calls to time both the provided serial implementation, and your parallel kernel<br>Note that because of the large number of atomic operations, your code may well be slower than the serial version. This does not neccessarily indicate a problem with your submission for this exercise.</li>
</ul>
<h2 id="Exercise-2-Scheduling-1-mark"><a href="#Exercise-2-Scheduling-1-mark" class="headerlink" title="Exercise 2 - Scheduling (1 mark)"></a>Exercise 2 - Scheduling (1 mark)</h2><p>As before, we would like to relax the assumption that the number of threads always equals <code>n</code>. For this task, implement two additional kernels; one which implements a block distribution of array indices, and one which implements a cyclic distribution of array indices. Also record which of these kernels is more performant, utilising a suitable experiment. Describe your experimental procedure, your results, and a brief explanation of why the more performant kernel performs better in the provided <code>dot_prod_observations.md</code> file.  </p>
<h2 id="Exercise-3-Shared-memory-1-mark"><a href="#Exercise-3-Shared-memory-1-mark" class="headerlink" title="Exercise 3 - Shared memory (1 mark)"></a>Exercise 3 - Shared memory (1 mark)</h2><p>Finally, we explore one more method of increasing performance, in addition to the warp shuffle described in the lab. CUDA provides the ability for an array to be shared using local (i.e on-card) memory, which allows for an alternative method of accessing values held by other threads. Declaring such an array should be done in the kernel, and can be done by utilising the <code>__shared__</code> keyword. For instance <code>__shared__ int my_shared_int_array[10];</code> declares a shared array with 10 ints inside. Note that the number of elements in the array must be known at compile-time; you can’t dynamically allocate these arrays.<br>One other piece of information that might be useful depending on how you do this exercise is that you can implement a barrier for all the threads in a block using <code>__syncthreads()</code>.<br>You may also make the same assumption as when we were doing the warp shuffles, that a block will always have 32 threads.<br>Utilising shared arrays to access values held by other threads in a warp, implement an additional kernel using the more performant scheduling method that performs better than the kernels from exercise 2. </p>
<p>NB: You might need to use large values for <code>n</code> and compute the dot product many times in order to see a performance difference. Try experimenting with values that make the CUDA version take around a second.</p>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html">CUDA Programming Guide</a></p>

</div> 

<script>
    window.onload = detectors();
</script>
    <div class="post-footer">
    <div class="h-line-primary"></div>
    <nav class="post-nav">
        <div class="prev-item">
           
                <div class="icon arrow-left"></div>
                <div class="post-link">
                    <a href="/2023/02/07/index/">Prev</a>
                </div>
            
        </div>
        <div class="next-item">
            
                <div class="icon arrow-right"></div>
                <div class="post-link">
                  <a href="/2022/05/04/Keras/">Next</a>  
                </div>  
            
        </div>
    </nav>
</div>

    
      <div class="post-comment">

     

     
    
    

</div>
     
  
</article>
        </div>
      </div>
      
      <div class="footer">
    <div class="flex-container">
        <div class="footer-text">
            
            
                 | 
            
                
        </div>
    </div>
</div>

    </div>

  </body>
</html>
